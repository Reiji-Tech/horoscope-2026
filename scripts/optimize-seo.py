#!/usr/bin/env python3
"""
SEOè‡ªå‹•æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¡ã‚¿ã‚¿ã‚°ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã€OGPç”»åƒãªã©ã‚’è‡ªå‹•è¿½åŠ 
"""

import os
import json
from pathlib import Path
from bs4 import BeautifulSoup
from datetime import datetime

# ã‚µã‚¤ãƒˆåŸºæœ¬æƒ…å ±
SITE_URL = "https://reiji-tech.github.io/horoscope-2026"
SITE_NAME = "2026å¹´å ã„ - 12æ˜Ÿåº§åˆ¥å®Œå…¨ã‚¬ã‚¤ãƒ‰"
SITE_DESCRIPTION = "2026å¹´ã®é‹å‹¢ã‚’12æ˜Ÿåº§åˆ¥ã«å®Œå…¨è§£èª¬ã€‚æ‹æ„›ã€ä»•äº‹ã€é‡‘é‹ã€å¥åº·é‹ã‚’è©³ã—ãå ã„ã¾ã™ã€‚"

# æ˜Ÿåº§æƒ…å ±
ZODIAC_INFO = {
    'aries': {'ja': 'ç‰¡ç¾Šåº§', 'en': 'Aries', 'date': '3/21-4/19', 'emoji': 'â™ˆ'},
    'taurus': {'ja': 'ç‰¡ç‰›åº§', 'en': 'Taurus', 'date': '4/20-5/20', 'emoji': 'â™‰'},
    'gemini': {'ja': 'åŒå­åº§', 'en': 'Gemini', 'date': '5/21-6/21', 'emoji': 'â™Š'},
    'cancer': {'ja': 'èŸ¹åº§', 'en': 'Cancer', 'date': '6/22-7/22', 'emoji': 'â™‹'},
    'leo': {'ja': 'ç…å­åº§', 'en': 'Leo', 'date': '7/23-8/22', 'emoji': 'â™Œ'},
    'virgo': {'ja': 'ä¹™å¥³åº§', 'en': 'Virgo', 'date': '8/23-9/22', 'emoji': 'â™'},
    'libra': {'ja': 'å¤©ç§¤åº§', 'en': 'Libra', 'date': '9/23-10/23', 'emoji': 'â™'},
    'scorpio': {'ja': 'è åº§', 'en': 'Scorpio', 'date': '10/24-11/22', 'emoji': 'â™'},
    'sagittarius': {'ja': 'å°„æ‰‹åº§', 'en': 'Sagittarius', 'date': '11/23-12/21', 'emoji': 'â™'},
    'capricorn': {'ja': 'å±±ç¾Šåº§', 'en': 'Capricorn', 'date': '12/22-1/19', 'emoji': 'â™‘'},
    'aquarius': {'ja': 'æ°´ç“¶åº§', 'en': 'Aquarius', 'date': '1/20-2/18', 'emoji': 'â™’'},
    'pisces': {'ja': 'é­šåº§', 'en': 'Pisces', 'date': '2/19-3/20', 'emoji': 'â™“'},
}

def create_structured_data(page_type, zodiac_sign=None):
    """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON-LDï¼‰ã‚’ç”Ÿæˆ"""
    
    if page_type == 'homepage':
        return {
            "@context": "https://schema.org",
            "@type": "WebSite",
            "name": SITE_NAME,
            "url": SITE_URL,
            "description": SITE_DESCRIPTION,
            "publisher": {
                "@type": "Organization",
                "name": "Horoscope 2026"
            }
        }
    
    elif page_type == 'article' and zodiac_sign:
        info = ZODIAC_INFO.get(zodiac_sign, {})
        return {
            "@context": "https://schema.org",
            "@type": "Article",
            "headline": f"ã€2026å¹´é‹å‹¢ã€‘{info.get('ja', '')}ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰",
            "description": f"{info.get('ja', '')}({info.get('date', '')})ã®2026å¹´é‹å‹¢ã€‚æ‹æ„›é‹ã€ä»•äº‹é‹ã€é‡‘é‹ã€å¥åº·é‹ã‚’è©³ã—ãè§£èª¬ã€‚",
            "author": {
                "@type": "Organization",
                "name": "Horoscope 2026"
            },
            "publisher": {
                "@type": "Organization",
                "name": "Horoscope 2026"
            },
            "datePublished": "2026-01-01",
            "dateModified": datetime.now().strftime("%Y-%m-%d"),
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": f"{SITE_URL}/{zodiac_sign}.html"
            }
        }
    
    return None

def optimize_html_file(html_path):
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®SEOæœ€é©åŒ–"""
    print(f"SEOæœ€é©åŒ–ä¸­: {html_path}")
    
    with open(html_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    head = soup.find('head')
    
    if not head:
        print(f"  âš ï¸  <head>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ˜Ÿåº§ã‚’ç‰¹å®š
    filename = Path(html_path).stem
    zodiac_sign = filename if filename in ZODIAC_INFO else None
    is_homepage = filename in ['index', 'home']
    
    # 1. ãƒ¡ã‚¿ã‚¿ã‚°ã®è¿½åŠ ãƒ»æ›´æ–°
    if zodiac_sign:
        info = ZODIAC_INFO[zodiac_sign]
        title = f"ã€2026å¹´é‹å‹¢ã€‘{info['ja']}ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ | æ‹æ„›ãƒ»ä»•äº‹ãƒ»é‡‘é‹"
        description = f"{info['ja']}({info['date']})ã®2026å¹´é‹å‹¢ã‚’è©³ã—ãè§£èª¬ã€‚æ‹æ„›é‹ã€ä»•äº‹é‹ã€é‡‘é‹ã€å¥åº·é‹ã‚’å ã„ã¾ã™ã€‚ãƒ©ãƒƒã‚­ãƒ¼ã‚«ãƒ©ãƒ¼ã‚„é–‹é‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚ã€‚"
        keywords = f"{info['ja']},2026å¹´é‹å‹¢,{info['en']},å ã„,ãƒ›ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—,æ‹æ„›é‹,ä»•äº‹é‹,é‡‘é‹"
    else:
        title = SITE_NAME
        description = SITE_DESCRIPTION
        keywords = "å ã„,2026å¹´,é‹å‹¢,ãƒ›ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—,12æ˜Ÿåº§,æ‹æ„›é‹,ä»•äº‹é‹,é‡‘é‹"
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    if not soup.find('title'):
        title_tag = soup.new_tag('title')
        title_tag.string = title
        head.append(title_tag)
        print(f"  âœ… ã‚¿ã‚¤ãƒˆãƒ«è¿½åŠ ")
    
    # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if not meta_desc:
        meta_desc = soup.new_tag('meta', attrs={'name': 'description', 'content': description})
        head.append(meta_desc)
        print(f"  âœ… ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if not soup.find('meta', attrs={'name': 'keywords'}):
        meta_keywords = soup.new_tag('meta', attrs={'name': 'keywords', 'content': keywords})
        head.append(meta_keywords)
        print(f"  âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ")
    
    # viewport
    if not soup.find('meta', attrs={'name': 'viewport'}):
        meta_viewport = soup.new_tag('meta', attrs={
            'name': 'viewport',
            'content': 'width=device-width, initial-scale=1.0'
        })
        head.append(meta_viewport)
        print(f"  âœ… Viewportè¿½åŠ ")
    
    # 2. OGPï¼ˆOpen Graph Protocolï¼‰ã‚¿ã‚°
    og_tags = {
        'og:title': title,
        'og:description': description,
        'og:type': 'article' if zodiac_sign else 'website',
        'og:url': f"{SITE_URL}/{filename}.html",
        'og:site_name': SITE_NAME,
        'og:locale': 'ja_JP',
    }
    
    for property_name, content in og_tags.items():
        if not soup.find('meta', attrs={'property': property_name}):
            og_tag = soup.new_tag('meta', attrs={'property': property_name, 'content': content})
            head.append(og_tag)
    
    print(f"  âœ… OGPã‚¿ã‚°è¿½åŠ ")
    
    # 3. Twitter Card
    twitter_tags = {
        'twitter:card': 'summary_large_image',
        'twitter:title': title,
        'twitter:description': description,
    }
    
    for name, content in twitter_tags.items():
        if not soup.find('meta', attrs={'name': name}):
            twitter_tag = soup.new_tag('meta', attrs={'name': name, 'content': content})
            head.append(twitter_tag)
    
    print(f"  âœ… Twitterã‚«ãƒ¼ãƒ‰è¿½åŠ ")
    
    # 4. æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON-LDï¼‰
    page_type = 'article' if zodiac_sign else 'homepage'
    structured_data = create_structured_data(page_type, zodiac_sign)
    
    if structured_data and not soup.find('script', attrs={'type': 'application/ld+json'}):
        script_tag = soup.new_tag('script', type='application/ld+json')
        script_tag.string = json.dumps(structured_data, ensure_ascii=False, indent=2)
        head.append(script_tag)
        print(f"  âœ… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿è¿½åŠ ")
    
    # 5. ã‚«ãƒãƒ‹ã‚«ãƒ«URL
    if not soup.find('link', attrs={'rel': 'canonical'}):
        canonical = soup.new_tag('link', attrs={
            'rel': 'canonical',
            'href': f"{SITE_URL}/{filename}.html"
        })
        head.append(canonical)
        print(f"  âœ… ã‚«ãƒãƒ‹ã‚«ãƒ«URLè¿½åŠ ")
    
    # 6. è¨€èªæŒ‡å®š
    html_tag = soup.find('html')
    if html_tag and not html_tag.get('lang'):
        html_tag['lang'] = 'ja'
        print(f"  âœ… è¨€èªå±æ€§è¿½åŠ ")
    
    # 7. æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    if not soup.find('meta', attrs={'charset': True}):
        charset_meta = soup.new_tag('meta', attrs={'charset': 'UTF-8'})
        head.insert(0, charset_meta)
        print(f"  âœ… æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(str(soup.prettify()))
    
    print(f"  âœ¨ SEOæœ€é©åŒ–å®Œäº†\n")
    return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ” SEOè‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print()
    
    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    html_files = list(Path('.').rglob('*.html'))
    html_files = [
        f for f in html_files 
        if not any(exclude in str(f) for exclude in ['node_modules', '.git', 'vendor'])
    ]
    
    print(f"å¯¾è±¡HTMLãƒ•ã‚¡ã‚¤ãƒ«: {len(html_files)}å€‹\n")
    
    for html_file in html_files:
        optimize_html_file(html_file)
    
    print("=" * 60)
    print("âœ… SEOæœ€é©åŒ–å®Œäº†")
    print("=" * 60)

if __name__ == '__main__':
    main()
